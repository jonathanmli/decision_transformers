{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Transformers Replication Report\n",
    "\n",
    "The following project seeks to replicate the gym results of the following paper:\n",
    "\n",
    "https://arxiv.org/abs/2106.01345\n",
    "\n",
    "The offline RL data, data parsing code, and some model parameters are taken from their github:\n",
    "\n",
    "https://github.com/kzl/decision-transformer\n",
    "\n",
    "This project contains the following files:\n",
    "\n",
    "- `RLagents.py` general framework for RL agents\n",
    "- `jonathans_experiment.py` code for running experiments and sampling from datasets\n",
    "- `DTagents.py` framework for decision transformer agents\n",
    "- `models.py` contains the neural networks used\n",
    "- `data` directory containing the offline RL datasets, which can be obtained by following directions on their github repo or from d4rl\n",
    "\n",
    "In addition, make sure that the pytorch, huggingface, and mujoco libraries are in your environment. Instructions to download them can be found on their corresponding websites.\n",
    "\n",
    "Below are some step by step instructions on how to use these files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from jonathans_experiment import *\n",
    "from DTagents import *\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "==================================================\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "# make the experiment environment and dt agent\n",
    "new_batch, env, max_ep_len, scale, env_target = prepare_experiment('gym-experiment', device='cpu')\n",
    "dta = DecisionTransformerAgent(env, scale=scale, target_return=env_target, warmup_steps=100, warmup_method=1, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 17.9004123210907\n"
     ]
    }
   ],
   "source": [
    "# train the agent\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    s, a, r, d, rtg, timesteps, mask = new_batch(64)\n",
    "    dta.offline_train(s, a, r, d, rtg, timesteps, mask, per_batch=True)\n",
    "print('training time', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmzhl/gym/DTagents.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.cat((a, torch.tensor(action, dtype=self.dtype, device=self.device).reshape(1, -1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation time 15.248412609100342\n",
      "mean return 57.246616615053775\n",
      "std returns 3.1431324642228016\n",
      "mean lengths 57.246616615053775\n",
      "std lengths 3.1431324642228016\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent and compute statistics\n",
    "start_time = time.time()\n",
    "returns, lengths = dta.online_evaluate(100)\n",
    "print('evaluation time', time.time() - start_time)\n",
    "print('mean return', returns.mean())\n",
    "print('std returns', returns.std())\n",
    "print('mean lengths', returns.mean())\n",
    "print('std lengths', returns.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark \n",
    "For comparison, below are the results after running the experiment using the author's code for 100 iterations with the same parameters\n",
    "\n",
    "- time/training: 41.14713501930237\n",
    "- evaluation/target_3600_return_mean: 42.6457553131806\n",
    "- evaluation/target_3600_return_std: 1.6768748119913417\n",
    "- evaluation/target_3600_length_mean: 27.79\n",
    "- evaluation/target_3600_length_std: 0.8401785524517987\n",
    "\n",
    "- time/total: 63.860310554504395\n",
    "- time/evaluation: 22.713170051574707\n",
    "- training/train_loss_mean: 0.6614395618438721\n",
    "- training/train_loss_std: 0.02305582663767387\n",
    "- training/action_error: 0.6420342326164246\n",
    "\n",
    "Seems like they probably used additional methods than those mentioned in the paper to reduce the variance, but otherwise the results look similiar\n",
    "\n",
    "# Results for larger experiments\n",
    "\n",
    "Below are results when I run the experiment for 1000 iterations with warmup steps = 1000 and lr=0.0001, holding everything else constant\n",
    "- Training time:  408.6037516593933\n",
    "- Training time per batch 0.40860375165939333\n",
    "- mean return 86.40965465081138\n",
    "- std returns 11.182001940365332\n",
    "- mean lengths 86.40965465081138\n",
    "- std lengths 11.182001940365332\n",
    "- Testing time:  24.500950574874878\n",
    "\n",
    "These results are very similiar to those presented in the paper, where they run the experiment for 100000 iterations and 100000 warmup steps instead. However, we do see a higher std than in the results in the paper, as expected with our lower sample size.\n",
    "\n",
    "Unfortunately I did not have the computational resources to run the experiment for 100000 at the current moment, but I expect the results to be similiar. Neither could I run the benchmark for 1000 iterations since it took significantly more computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for further research\n",
    "- The authors do not seem to have normalized their returns per episode using the method from d4rl like they claimed\n",
    "- In their episode evaluation, the authors do not seem to have used scaled returns to go, which was used during training. In our replication we scale the rtg in evaluation\n",
    "- Currently the transformer processes reward to go, state, and action tokens similiarly. I think that using an architecture that differientiates between them would improve performance\n",
    "- Using a better prediction layer than simply single layer linear prediction might result in better action predictions\n",
    "- In evaluation, the unknown next action is currently padded as a zero dimensional vector. This does not indicate to the model that we are trying to predict the unknown rather than translate. I think make modifications on this might be useful\n",
    "- It might be interesting to try to have the model predict future action/state/reward sequences as well in other to create context which can then be used to predict the current action\n",
    "- The loss is currently based on how similiar the predicted actions are to their actual actions. This means that the model is incentivized to stick to existing action sequences, and also that the loss is not based on the reward earned. Having the loss include rewards might incentivize the model to innovate new sequences and thus improve performance\n",
    "- I tried adding more warmup methods but they did not seem to be of great effect\n",
    "\n",
    "I think that these would be interesting improvements to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
