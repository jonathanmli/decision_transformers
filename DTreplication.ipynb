{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Transformers Replication Report\n",
    "\n",
    "The following project seeks to replicate the gym results of the following paper:\n",
    "\n",
    "https://arxiv.org/abs/2106.01345\n",
    "\n",
    "The offline RL data, data parsing code, and some model parameters are taken from their github:\n",
    "\n",
    "https://github.com/kzl/decision-transformer\n",
    "\n",
    "This project contains the following files:\n",
    "\n",
    "- `RLagents.py` general framework for RL agents\n",
    "- `jonathans_experiment.py` code for running experiments and sampling from datasets\n",
    "- `DTagents.py` framework for decision transformer agents\n",
    "- `models.py` contains the neural networks used\n",
    "- `data` directory containing the offline RL datasets, which can be obtained by following directions on their github repo or from d4rl\n",
    "\n",
    "In addition, make sure that the pytorch, huggingface, and mujoco libraries are in your environment. Instructions to download them can be found on their corresponding websites.\n",
    "\n",
    "Below are some step by step instructions on how to use these files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from jonathans_experiment import *\n",
    "from DTagents import *\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "==================================================\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "# make the experiment environment and dt agent\n",
    "new_batch, env, max_ep_len, scale, env_target, state_mean, state_std = prepare_experiment('gym-experiment', device='cpu')\n",
    "dta = DecisionTransformerAgent(env, scale=scale, target_return=env_target, warmup_steps=100, warmup_method=1, \n",
    "                               lr=0.01, state_mean=state_mean, state_std=state_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time 13.977554559707642\n"
     ]
    }
   ],
   "source": [
    "# train the agent\n",
    "start_time = time.time()\n",
    "for i in range(10):\n",
    "    s, a, r, d, rtg, timesteps, mask = new_batch(64)\n",
    "    dta.offline_train(s, a, r, d, rtg, timesteps, mask, per_batch=True)\n",
    "print('training time', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmzhl/gym/DTagents.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.cat((a, torch.tensor(action, dtype=self.dtype, device=self.device).reshape(1, -1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps 1\n",
      "eps 2\n",
      "eps 3\n",
      "eps 4\n",
      "eps 5\n",
      "eps 6\n",
      "eps 7\n",
      "eps 8\n",
      "eps 9\n",
      "eps 10\n",
      "eps 11\n",
      "eps 12\n",
      "eps 13\n",
      "eps 14\n",
      "eps 15\n",
      "eps 16\n",
      "eps 17\n",
      "eps 18\n",
      "eps 19\n",
      "eps 20\n",
      "eps 21\n",
      "eps 22\n",
      "eps 23\n",
      "eps 24\n",
      "eps 25\n",
      "eps 26\n",
      "eps 27\n",
      "eps 28\n",
      "eps 29\n",
      "eps 30\n",
      "eps 31\n",
      "eps 32\n",
      "eps 33\n",
      "eps 34\n",
      "eps 35\n",
      "eps 36\n",
      "eps 37\n",
      "eps 38\n",
      "eps 39\n",
      "eps 40\n",
      "eps 41\n",
      "eps 42\n",
      "eps 43\n",
      "eps 44\n",
      "eps 45\n",
      "eps 46\n",
      "eps 47\n",
      "eps 48\n",
      "eps 49\n",
      "eps 50\n",
      "eps 51\n",
      "eps 52\n",
      "eps 53\n",
      "eps 54\n",
      "eps 55\n",
      "eps 56\n",
      "eps 57\n",
      "eps 58\n",
      "eps 59\n",
      "eps 60\n",
      "eps 61\n",
      "eps 62\n",
      "eps 63\n",
      "eps 64\n",
      "eps 65\n",
      "eps 66\n",
      "eps 67\n",
      "eps 68\n",
      "eps 69\n",
      "eps 70\n",
      "eps 71\n",
      "eps 72\n",
      "eps 73\n",
      "eps 74\n",
      "eps 75\n",
      "eps 76\n",
      "eps 77\n",
      "eps 78\n",
      "eps 79\n",
      "eps 80\n",
      "eps 81\n",
      "eps 82\n",
      "eps 83\n",
      "eps 84\n",
      "eps 85\n",
      "eps 86\n",
      "eps 87\n",
      "eps 88\n",
      "eps 89\n",
      "eps 90\n",
      "eps 91\n",
      "eps 92\n",
      "eps 93\n",
      "eps 94\n",
      "eps 95\n",
      "eps 96\n",
      "eps 97\n",
      "eps 98\n",
      "eps 99\n",
      "evaluation time 1367.6661880016327\n",
      "mean return 136.6401590305296\n",
      "std returns 6.777549149085109\n",
      "mean lengths 136.6401590305296\n",
      "std lengths 6.777549149085109\n"
     ]
    }
   ],
   "source": [
    "# evaluate the agent and compute statistics\n",
    "start_time = time.time()\n",
    "returns, lengths = dta.online_evaluate(100)\n",
    "print('evaluation time', time.time() - start_time)\n",
    "print('mean return', returns.mean())\n",
    "print('std returns', returns.std())\n",
    "print('mean lengths', returns.mean())\n",
    "print('std lengths', returns.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3d2b98929b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate the agent using their evaluation code and compute statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# state_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm_online_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gym/DTagents.py\u001b[0m in \u001b[0;36mbm_online_evaluate\u001b[0;34m(self, num_eval_episodes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0meval_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/DTagents.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    156\u001b[0m                             \u001b[0mstate_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                             \u001b[0mstate_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                         )\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/DTagents.py\u001b[0m in \u001b[0;36mevaluate_episode_rtg\u001b[0;34m(env, state_dim, act_dim, model, max_ep_len, scale, state_mean, state_std, device, target_return, mode)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mtarget_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mtimesteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         )\n\u001b[1;32m    227\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/models.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, states, actions, rewards, returns_to_go, timesteps, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         _, action_preds, return_preds = self.forward(\n\u001b[0;32m--> 146\u001b[0;31m             returns_to_go, states, actions, timesteps, mask=attention_mask, **kwargs)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, rtg, states, actions, timesteps, mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# use transformer to get hidden states from output dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# this should be a tensor of size Z * 3K * Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPT2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;31m#         print('hs', hidden_states)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m#         print('hs s', hidden_states.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jjj/lib/python3.7/site-packages/torch-1.9.0-py3.7-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jjj/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jjj/lib/python3.7/site-packages/torch-1.9.0-py3.7-linux-x86_64.egg/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jjj/lib/python3.7/site-packages/torch-1.9.0-py3.7-linux-x86_64.egg/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jjj/lib/python3.7/site-packages/torch-1.9.0-py3.7-linux-x86_64.egg/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# evaluate the agent using their evaluation code and compute statistics\n",
    "# state_mean\n",
    "dta.bm_online_evaluate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark \n",
    "For comparison, below are the results after running the experiment using the author's code for 100 iterations with the same parameters\n",
    "\n",
    "- time/training: 41.14713501930237\n",
    "- evaluation/target_3600_return_mean: 42.6457553131806\n",
    "- evaluation/target_3600_return_std: 1.6768748119913417\n",
    "- evaluation/target_3600_length_mean: 27.79\n",
    "- evaluation/target_3600_length_std: 0.8401785524517987\n",
    "\n",
    "- time/total: 63.860310554504395\n",
    "- time/evaluation: 22.713170051574707\n",
    "- training/train_loss_mean: 0.6614395618438721\n",
    "- training/train_loss_std: 0.02305582663767387\n",
    "- training/action_error: 0.6420342326164246\n",
    "\n",
    "Seems like they probably used additional methods than those mentioned in the paper to reduce the variance, but otherwise the results look similiar\n",
    "\n",
    "# Results for larger experiments\n",
    "\n",
    "Below are results when I run the experiment for 1000 iterations with warmup steps = 1000 and lr=0.0001, holding everything else constant\n",
    "- Training time:  408.6037516593933\n",
    "- Training time per batch 0.40860375165939333\n",
    "- mean return 86.40965465081138\n",
    "- std returns 11.182001940365332\n",
    "- mean lengths 86.40965465081138\n",
    "- std lengths 11.182001940365332\n",
    "- Testing time:  24.500950574874878\n",
    "\n",
    "These results are very similiar to those presented in the paper, where they run the experiment for 100000 iterations and 100000 warmup steps instead. However, we do see a higher std than in the results in the paper, as expected with our lower sample size.\n",
    "\n",
    "Unfortunately I did not have the computational resources to run the experiment for 100000 at the current moment, but I expect the results to be similiar. Neither could I run the benchmark for 1000 iterations since it took significantly more computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes for further research\n",
    "- The authors do not seem to have normalized their returns per episode using the method from d4rl like they claimed\n",
    "- In their episode evaluation, the authors do not seem to have used scaled returns to go, which was used during training. In our replication we scale the rtg in evaluation\n",
    "- Currently the transformer processes reward to go, state, and action tokens similiarly. I think that using an architecture that differientiates between them would improve performance\n",
    "- Using a better prediction layer than simply single layer linear prediction might result in better action predictions\n",
    "- In evaluation, the unknown next action is currently padded as a zero dimensional vector. This does not indicate to the model that we are trying to predict the unknown rather than translate. I think make modifications on this might be useful\n",
    "- It might be interesting to try to have the model predict future action/state/reward sequences as well in other to create context which can then be used to predict the current action\n",
    "- The loss is currently based on how similiar the predicted actions are to their actual actions. This means that the model is incentivized to stick to existing action sequences, and also that the loss is not based on the reward earned. Having the loss include rewards might incentivize the model to innovate new sequences and thus improve performance\n",
    "- I tried adding more warmup methods but they did not seem to be of great effect\n",
    "\n",
    "I think that these would be interesting improvements to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
